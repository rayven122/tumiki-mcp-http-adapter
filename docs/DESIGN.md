# Tumiki MCP HTTP Adapter - システムアーキテクチャ設計書

**Languages**: 日本語 | [🇬🇧 English](DESIGN_EN.md)

## システムアーキテクチャ

### システム構成図

```
┌─────────────┐
│   Client    │
└──────┬──────┘
       │ HTTP Request
       │ (Custom Headers: X-Slack-Token, X-Team-Id, etc.)
       ▼
┌─────────────────────────────────────┐
│   Tumiki MCP HTTP Adapter           │
│                                     │
│  ┌──────────────────────────────┐  │
│  │  Proxy Handler               │  │
│  │  - Header Mapping            │  │
│  │  - Env/Args Building         │  │
│  └───────────┬──────────────────┘  │
│              ▼                      │
│  ┌──────────────────────────────┐  │
│  │  Process Executor            │  │
│  │  - Stdio Process Launch      │  │
│  │  - Input/Output Handling     │  │
│  └──────────────────────────────┘  │
└─────────────┬───────────────────────┘
              │
              ▼
      ┌──────────────┐
      │ MCP Server   │
      │ (stdio mode) │
      └──────────────┘
```

### データフロー

1. **リクエスト受信**: HTTP POST /mcp
2. **ヘッダー解析**: カスタムマッピングに基づいて環境変数・引数を抽出
3. **設定マージ**: デフォルト環境変数 + ヘッダー由来の値
4. **プロセス起動**: stdio モードで MCP サーバーを実行
5. **レスポンス返却**: MCP サーバーの出力を HTTP レスポンスとして返却

---

## コンポーネント設計

### 1. cmd/tumiki-mcp-http

**責務**: アプリケーションエントリーポイント、CLI 解析

**主要機能**:

- コマンドラインフラグの定義と解析
- 設定のビルド
- サーバーの起動とシャットダウン処理
- シグナルハンドリング（Graceful Shutdown）

**設計上の重要ポイント**:

- `ArrayFlags` 型で複数回指定可能なフラグを実装
- `parseStdioCommand()` でシェルスタイルのコマンド文字列を解析（クォート対応）
- `buildConfigFromFlags()` で CLI フラグから設定を構築
- `startServer()` で defer + exitCode パターンにより Graceful Shutdown 実現

### 2. internal/proxy

**責務**: HTTP サーバー、MCP エンドポイントハンドラー、ヘッダー解析

**主要データ構造**:

- **Config**: サーバー設定
  - ポート番号
  - stdio コマンドと引数
  - デフォルト環境変数
  - ヘッダー→環境変数マッピング
  - ヘッダー→引数マッピング

- **タイムアウト定数**:
  - ReadTimeout: 30秒（HTTPリクエスト読み取り）
  - WriteTimeout: 30秒（HTTPレスポンス書き込み）
  - ShutdownTimeout: 5秒（Graceful Shutdown）
  - ProcessTimeout: 30秒（stdioプロセス実行）

- **Server**: HTTPサーバーインスタンス
  - 設定（Config）
  - 構造化ロガー
  - HTTPサーバー本体

**公開 API**:

- `NewServer`: サーバーインスタンスの生成
- `Start`: サーバーの起動（Context対応）
- `Handler`: HTTPハンドラーの取得（テスト用）

**内部関数**:

- `handleMCP`: MCP HTTPエンドポイントハンドラー
- `parseHeaders`: HTTPヘッダーから環境変数と引数を抽出

**処理フロー（handleMCP）**:

1. `parseHeaders()` でヘッダーを解析
2. デフォルト環境変数とマージ
3. 引数をマージ（元のスライスは変更しない - appendAssign 対策）
4. リクエストボディ読み込み
5. プロセス実行（タイムアウト付き）
6. レスポンス返却（エラーハンドリング付き）

**設計上の重要ポイント**:

- `parseHeaders()` は純粋関数として実装（テスト容易性）
- 引数マージ時に元のスライスを変更しない（並行処理の安全性）
- エラーレスポンスは適切な HTTP ステータスコードと共に返却
- Context 伝播によりクライアント切断時の適切なクリーンアップ

### 3. internal/process

**責務**: stdio プロセスの起動と入出力処理

**主要データ構造**:

- **Executor**: プロセス実行管理
  - コマンド名
  - コマンド引数
  - 環境変数マップ
  - 構造化ロガー

**公開 API**:

- `NewExecutor`: Executor インスタンスの生成
- `Execute`: プロセス実行と入出力処理（Context対応）

**処理フロー（Execute）**:

1. `exec.CommandContext` でプロセス作成
2. 環境変数設定
3. stdin/stdout/stderr パイプ接続
4. プロセス起動
5. stderr を非同期で読み取り（sync.WaitGroup でデータレース防止）
6. 入力データを stdin に書き込み
7. stdin をクローズ
8. stdout から JSON-RPC レスポンス読み取り
9. プロセス終了待機
10. stderr 読み取り完了待機（WaitGroup.Wait）
11. エラーハンドリング（stderr の内容をログ出力）
12. 出力データ返却

**設計上の重要ポイント**:

- **データレース対策**: `sync.WaitGroup` で stderr の非同期読み取りを同期
- **リソース管理**: defer や明示的な Close() でリソースリーク防止
- **Context 伝播**: `exec.CommandContext` でタイムアウト・キャンセル対応
- **エラーログ**: プロセス失敗時に stderr の内容を構造化ログで出力

---

## 動的ヘッダーマッピング設計

### Streamable HTTP パターン

従来の固定設定と異なり、HTTP リクエストごとに異なる環境変数・引数を動的に設定できる設計。

### 設計原理

**マッピング定義（CLI 起動時）**:

```
X-Slack-Token → SLACK_TOKEN (環境変数)
X-Team-Id     → team-id (引数)
X-Channel     → channel (引数)
```

**実行時変換（HTTP リクエスト時）**:

HTTPリクエストで以下のヘッダーを受信した場合:
```
X-Slack-Token: xoxp-12345
X-Team-Id: T123
X-Channel: general
```

マッピング定義に従って、以下のように変換されます:
```
環境変数: SLACK_TOKEN=xoxp-12345
引数: --team-id T123 --channel general

実行コマンド:
npx -y server-slack --team-id T123 --channel general
```

### 設計上のメリット

1. **動的設定**: リクエストごとに異なるトークン・引数を使用可能
2. **マルチテナント対応**: チームやユーザーごとに異なる認証情報
3. **セキュリティ**: 機密情報をコマンドライン引数ではなく環境変数で渡せる
4. **柔軟性**: ヘッダー名を完全に自由に定義可能

---

## エラーハンドリング設計

### HTTP エラーレスポンス

| ステータスコード          | 用途           | 発生条件                       |
| ------------------------- | -------------- | ------------------------------ |
| 200 OK                    | 正常処理       | プロセス実行成功               |
| 400 Bad Request           | リクエスト不正 | ボディ読み込み失敗             |
| 500 Internal Server Error | サーバーエラー | プロセス実行失敗・タイムアウト |

### ログ設計

**構造化ログ（slog）**:

標準ライブラリの `slog` パッケージを使用した構造化ログを出力します。

ログ出力例:
```
INFO: Server starting addr=0.0.0.0:8080
ERROR: Process execution failed error="..." stderr="..."
DEBUG: Failed to copy stderr error="..."
```

**ログレベル使い分け**:

- `debug`: 非本質的なエラー（stderr コピー失敗、リソースクローズ失敗等）
- `info`: 通常の動作ログ（サーバー起動、シャットダウン等）
- `warn`: 警告（未使用）
- `error`: 重大なエラー（プロセス実行失敗、リクエスト処理失敗等）

---

## セキュリティ設計

### 脅威モデル

**対象としない脅威（外部実装を推奨）**:

- 認証・認可
- TLS/HTTPS
- レート制限

**対象とする脅威**:

- プロセスタイムアウト（DoS 対策）
- リソース枯渇（タイムアウト・Context 管理）
- 機密情報の漏洩（ログ出力制御）

### 保護機構

**1. タイムアウト制御**:

各操作に適切なタイムアウトを設定:
- **ReadTimeout**: 30秒（HTTPリクエスト読み取り）
- **WriteTimeout**: 30秒（HTTPレスポンス書き込み）
- **ProcessTimeout**: 30秒（stdioプロセス実行）
- **ShutdownTimeout**: 5秒（Graceful Shutdown）

**2. Context ベースのキャンセル**:

- HTTP リクエストの Context をプロセス実行に伝播
- クライアント切断時にプロセスも終了（`exec.CommandContext`）

**3. 環境変数の保護**:

- 機密情報（トークン等）は環境変数で渡す
- コマンドライン引数にトークンを含めない（プロセスリスト露出対策）
- ログに機密情報を出力しない（構造化ログの Debug レベル以外）

**4. プロセス分離**:

- リクエストごとに独立したプロセスを起動
- プロセス間での状態共有なし
- メモリ空間の完全分離

---

## パフォーマンス設計

### 並行処理

**HTTP サーバー**:

- Go の goroutine で自動的に並行リクエスト処理
- デフォルトで無制限の並行接続（外部でレート制限推奨）

**プロセス実行**:

- リクエストごとに独立したプロセス起動
- プロセス間での排他制御不要（ステートレス）

### リソース管理

**メモリ**:

- ストリーミング処理でメモリ使用量を抑制
- バッファサイズは Go のデフォルト（8192 バイト）

**プロセス**:

- リクエスト完了後、確実にプロセス終了
- Context キャンセル時も適切にクリーンアップ

**I/O**:

- stderr は非同期読み取り（goroutine）
- `sync.WaitGroup` でデータレース防止
- `cmd.Wait()` 前に WaitGroup.Wait() でデッドロック防止

### スケーリング

**水平スケーリング**:

- ステートレス設計のため容易
- ロードバランサーで複数インスタンスに分散可能

**垂直スケーリング**:

- CPU コア数に応じて並行処理数が増加
- メモリ使用量は起動プロセス数に比例

---

## テスト設計

### テスト戦略

**単体テスト（100%カバレッジ目標）**:

- `cmd/tumiki-mcp-http/main_test.go` - CLI フラグ解析、設定ビルド
- `internal/proxy/server_test.go` - HTTP ハンドラー、ヘッダー解析
- `internal/process/executor_test.go` - プロセス実行、データレース対策

**テスト方針**:

- テーブル駆動テスト（複数のテストケースを構造化）
- 正常系・異常系・エッジケースの網羅
- `httptest.NewRecorder` で HTTP テスト
- 実際のプロセス（echo, cat, sh 等）でテスト

### テストカバレッジ

**カバレッジ除外（統合テスト対象）**:

- `main()` 関数
- `startServer()` 関数（シグナルハンドリング含む）
- サーバー起動・シャットダウン処理

**カバレッジ対象（100%目標）**:

- 全ての純粋関数
- ヘッダー解析ロジック（`parseHeaders`, `parseMapping`, `parseEnvVars`等）
- プロセス実行ロジック（`Execute`, `envSlice`等）
- エラーハンドリング（全てのエラーパス）

### データレーステスト

Goの race detector を使用してデータレースを検出します。

テスト実行コマンド:
```bash
go test -race ./...
```

**検出・修正した事例**:
- `stderrBuf` への並行アクセス → `sync.WaitGroup` で同期

---

## 拡張性設計

### 外部統合ポイント

**1. リバースプロキシ（推奨）**:

- nginx, Caddy, Traefik 等
- 認証・認可
- TLS/HTTPS
- レート制限
- ロードバランシング

**2. 監視・ログ**:

- 構造化ログ（JSON）を外部システムに転送
- メトリクス収集（Prometheus 等）
- ヘルスチェック（/mcp への定期リクエスト）

**3. オーケストレーション**:

- Kubernetes でのデプロイ
- Docker Compose での複数サーバー管理
- Systemd での自動起動

### 設計上の拡張可能性

**現在の設計で対応可能**:

- 複数のヘッダーマッピング（無制限）
- 複数の環境変数（無制限）
- 任意の stdio プロセス

**外部実装が推奨される機能**:

- 認証・認可（OAuth, JWT 等）
- TLS/HTTPS（証明書管理）
- レート制限（Redis 等）
- 複数バックエンドのロードバランシング
- キャッシング

### シンプル設計の哲学

**原則**:

- 1 つのことをうまくやる（Unix 哲学）
- 複雑な機能は外部に委譲
- 小さく保守しやすいコードベース

**設計上のトレードオフ**:

**選択したこと**:

- シンプルさ > 機能豊富さ
- 外部統合 > 内部実装
- 標準ライブラリ > 外部依存

**選択しなかったこと**:

- 認証機能（外部リバースプロキシで実装）
- 設定ファイル（CLI フラグのみ）
- 複数サーバー対応（単一サーバー専用）
- ヘルスチェックエンドポイント（/mcp で代用可能）

---

## まとめ

### アーキテクチャの特徴

**構造**:

- 2 パッケージ構成（proxy, process）
- 標準ライブラリのみ（外部依存なし）
- 小さなコードベース（約 1000 行）

**品質**:

- 100%テストカバレッジ（テスト可能な関数）
- データレース防止（race detector 対応）
- golangci-lint 完全準拠（errcheck, gocritic 等）

**保守性**:

- 理解しやすい（小さく単純な設計）
- デバッグしやすい（構造化ログ、エラーハンドリング）
- 拡張しやすい（外部統合ポイント明確）
- 置き換えやすい（単一責務の原則）

この設計により、軽量で高性能、かつ保守性の高い HTTP プロキシを実現しています。
